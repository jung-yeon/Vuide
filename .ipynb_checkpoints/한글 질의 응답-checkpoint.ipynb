{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9cb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d58144",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"수원 화성은 언제 완성되었는가?\"\n",
    "context = \"\"\"수원 화성은 조선시대 화성유수부 시가지를 둘러싼 성곽이다.\n",
    "1789년(정조 13) 수원을 필달산 동쪽 아래로 옮기고,\n",
    "1794년(정조 18) 축성을 시작해 1796년에 완성했다.\"\"\"\n",
    "context = context.strip().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa97b7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9942862391471863, 'start': 85, 'end': 91, 'answer': '1796년에'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering, pipeline\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "model = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "question_answer = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
    "answer = question_answer({\n",
    "    \"question\":question,\n",
    "    \"context\":context\n",
    "})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba6e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: tensor(58) end: tensor(61)\n",
      "질의: 수원 화성은 언제 완성되었는가?\n",
      "응답: 1796년\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "print(\"start:\", answer_start, \"end:\", answer_end)\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환항고 다시 문자열로 변환\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(\"질의:\", question)\n",
    "print(\"응답:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f93e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "운전만해의 기능은 뭐가 있어?\n",
      "{'score': 0.9487451314926147, 'start': 87, 'end': 112, 'answer': 'TTS, STT, 노상주차 위치 서비스이다.이'}\n",
      "start: tensor(65) end: tensor(76)\n",
      "질의: 운전만해의 기능은 뭐가 있어?\n",
      "응답: TTS , STT , 노상주차 위치 서비스\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering, pipeline\n",
    "import torch\n",
    "\n",
    "question = input()\n",
    "context = \"\"\"\n",
    "운전만해의 목표는 교통사고 발생률을 감소시킨다.\n",
    "이 앱의 목표는 교통사고 발생률을 감소시킨다.\n",
    "이 어플의 목표는 교통사고 발생률을 감소시킨다.\n",
    "\n",
    "운전만해는 기능은 TTS, STT, 노상주차 위치 서비스이다.\n",
    "이 앱의 기능은 TTS, STT, 노상주차 위치 서비스이다.\n",
    "이 어플의기능은 TTS, STT, 노상주차 위치 서비스이다.\n",
    "\n",
    "TTS는 텍스트 음성 변환이다.\n",
    "\n",
    "STT는 음성 텍스트 변환이다.\n",
    "\n",
    "운전만해는 인공지능사관학교에서 탄생했다.\n",
    "이 앱은 인공지능사관학교에서 탄생했다.\n",
    "이 어플은 인공지능사관학교에서 탄생했다.\n",
    "\n",
    "운전만해는 Vuides팀이 만들었다.\n",
    "이 애플리케이션은 Vuides팀이 만들었다.\n",
    "이 앱은 Vuides팀이 만들었다.\n",
    "이 어플은 Vuides팀이 만들었다.\n",
    "\n",
    "운전만해는 문자읽기, 질의응답, 노상주차 위치 서비스를 제공한다.\n",
    "이 앱은 문자읽기, 질의응답, 노상주차 위치 서비스를 제공한다.\n",
    "이 어플은 문자읽기, 질의응답, 노상주차 위치 서비스를 제공한다.\n",
    "\n",
    "질의 응답은 CDQA방식이다.\n",
    "\n",
    "노상주차 위치 서비스의 구현은 공공데이터 포털을 통해 제작되었다.\n",
    "공공데이터는 '광주광역시의 공영 및 민영 주차장 현황'의 데이터를 이용하였다.\n",
    "\n",
    "운전만해의 기대효과는 교통사고 발생률 감소이다.\n",
    "이 앱의 기대효과는 교통사고 발생률 감소이다.\n",
    "이 어플의 기대효과는 교통사고 발생률 감소이다.\n",
    "\n",
    "운전만해의 활용분야는 보험료 할인 및 벌금 면제 등이 있다.\n",
    "이 앱의 활용분야는 보험료 할인 및 벌금 면제 등이 있다.\n",
    "이 어플의 활용분야는 보험료 할인 및 벌금 면제 등이 있다.\n",
    "\"\"\"\n",
    "context = context.strip().replace(\"\\n\", \"\")\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "model = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "question_answer = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
    "answer = question_answer({\n",
    "    \"question\":question,\n",
    "    \"context\":context\n",
    "})\n",
    "print(answer)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "print(\"start:\", answer_start, \"end:\", answer_end)\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환항고 다시 문자열로 변환\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(\"질의:\", question)\n",
    "print(\"응답:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489990cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering, pipeline\n",
    "import torch\n",
    "\n",
    "question = input()\n",
    "context = \"\"\"\n",
    "운전만해의 목표는 교통사고 발생률을 감소시킨다.\n",
    "이 앱의 목표는 교통사고 발생률을 감소시킨다.\n",
    "이 어플의 목표는 교통사고 발생률을 감소시킨다.\n",
    "\n",
    "운전만해는 기능은 TTS, STT, 노상주차 위치 서비스이다.\n",
    "이 앱의 기능은 TTS, STT, 노상주차 위치 서비스이다.\n",
    "이 어플의기능은 TTS, STT, 노상주차 위치 서비스이다.\n",
    "\n",
    "TTS는 텍스트 음성 변환이다.\n",
    "\n",
    "STT는 음성 텍스트 변환이다.\n",
    "\n",
    "운전만해는 인공지능사관학교에서 탄생했다.\n",
    "이 앱은 인공지능사관학교에서 탄생했다.\n",
    "이 어플은 인공지능사관학교에서 탄생했다.\n",
    "\n",
    "운전만해는 Vuides팀이 만들었다.\n",
    "이 애플리케이션은 Vuides팀이 만들었다.\n",
    "이 앱은 Vuides팀이 만들었다.\n",
    "이 어플은 Vuides팀이 만들었다.\n",
    "\n",
    "운전만해는 문자읽기, 질의응답, 노상주차 위치 서비스를 제공한다.\n",
    "이 앱은 문자읽기, 질의응답, 노상주차 위치 서비스를 제공한다.\n",
    "이 어플은 문자읽기, 질의응답, 노상주차 위치 서비스를 제공한다.\n",
    "\n",
    "질의 응답은 CDQA방식이다.\n",
    "\n",
    "노상주차 위치 서비스의 구현은 공공데이터 포털을 통해 제작되었다.\n",
    "공공데이터는 '광주광역시의 공영 및 민영 주차장 현황'의 데이터를 이용하였다.\n",
    "\n",
    "운전만해의 기대효과는 교통사고 발생률 감소이다.\n",
    "이 앱의 기대효과는 교통사고 발생률 감소이다.\n",
    "이 어플의 기대효과는 교통사고 발생률 감소이다.\n",
    "\n",
    "운전만해의 활용분야는 보험료 할인 및 벌금 면제 등이 있다.\n",
    "이 앱의 활용분야는 보험료 할인 및 벌금 면제 등이 있다.\n",
    "이 어플의 활용분야는 보험료 할인 및 벌금 면제 등이 있다.\n",
    "\"\"\"\n",
    "context = context.strip().replace(\"\\n\", \"\")\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "model = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "question_answer = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
    "answer = question_answer({\n",
    "    \"question\":question,\n",
    "    \"context\":context\n",
    "})\n",
    "print(answer)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "print(\"start:\", answer_start, \"end:\", answer_end)\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환항고 다시 문자열로 변환\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(\"응답:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64526d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "운전만해에서 온도를 알려주는 기능에 대해서 아려주]\n",
      "{'score': 0.9999783039093018, 'start': 215, 'end': 217, 'answer': '6로'}\n",
      "start: tensor(104) end: tensor(105)\n",
      "질의: 운전만해에서 온도를 알려주는 기능에 대해서 아려주]\n",
      "응답: 6\n",
      "최적 답변: 운전만해에서 현재 온도를 알려주는 방법에 대해서 설명하겠습니다. 먼저 브이즈를 호출한 뒤 '온도'라고 말하면 현재 온도 정보를 '권준오'님께 알려줍니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering, pipeline\n",
    "import torch\n",
    "\n",
    "# 한국어 Electra 모델과 토크나이저 불러오기\n",
    "model_name = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "model = ElectraForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Question-answering 파이프라인 생성\n",
    "question_answer = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 예시 질문과 맥락\n",
    "question = input()\n",
    "\n",
    "context = \"\"\"\n",
    "    날씨라는 호출로 현재 날씨 정보를 가져오는 것을 2로 한다.\n",
    "    \n",
    "    카카오톡이라는 호출로 친구에게 카카오톡 메시지를 전송하는 것을 3로 한다. \n",
    "    \n",
    "    주차장의 호출로 노상주차 알림 서비스를 제공하는 것을 4로 한다.\n",
    "    \n",
    "    시간이라는 호출로 현재 시간을 알려주는 것을 5로 한다.\n",
    "    \n",
    "    온도라는 호출로 현재 온도에 대해서 알려주는 것을 6로 한다.\n",
    "    \n",
    "    습도라는 호출로 현재 습도에 대해서 알려주는 것을 7로 한다.\"\"\"\n",
    "\n",
    "# 답 가져오기\n",
    "answer = question_answer({\n",
    "    \"question\": question,\n",
    "    \"context\": context\n",
    "})\n",
    "\n",
    "print(answer)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "print(\"start:\", answer_start, \"end:\", answer_end)\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환항고 다시 문자열로 변환\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(\"질의:\", question)\n",
    "print(\"응답:\", answer)\n",
    "\n",
    "\n",
    "user = \"권준오\"\n",
    "\n",
    "if '1' in answer:\n",
    "    answer = f\"\"\n",
    "elif '2' in answer:\n",
    "    answer = f\"운전만해에서 날씨 정보을 가져오는 방법에 대해서 설명하겠습니다. 먼저 브이즈를 호출한 뒤 '날씨'라고 말하면 현재 날씨에 대한 정보를 '{user}'님께 알려줍니다.\"\n",
    "elif '3' in answer:\n",
    "    answer = f\"운전만해에서 친구에게 카카오톡 메시지를 보내는 방법에 대해서 설명하겠습니다. 먼저 브이즈를 호출한 뒤, 메시지를 보낼 특정 사용자의 이름을 부릅니다. 그 후 상대방에게 보낼 메시지 내용을 말하면 상대방에게 텍스트로 발신됩니다.\"\n",
    "elif '4' in answer:\n",
    "    answer = f\"운전만해에서 노상주차 알림 서비스를 제공하는 방법에 대해서 설명하겠습니다. 먼저 브이즈를 호출한 뒤 '주차장'이라고 말하면 현재 위치에서 가장 근처인 노상 주차장의 정보를 '{user}'님께 알려줍니다.\"\n",
    "elif '5' in answer:\n",
    "    answer = f\"운전만해에서 현재 시간을 알려주는 방법에 대해서 설명하겠습니다. 먼저 브이즈를 호출한 뒤 '시간'이라고 말하면 현재 시간에 대한 정보를 '{user}'님께 알려줍니다.\"\n",
    "elif '6' in answer:\n",
    "    answer = f\"운전만해에서 현재 온도를 알려주는 방법에 대해서 설명하겠습니다. 먼저 브이즈를 호출한 뒤 '온도'라고 말하면 현재 온도 정보를 '{user}'님께 알려줍니다.\"\n",
    "elif '7' in answer:\n",
    "    answer = f\"운전만해에서 현재 습도를 알려주는 방법에 대해서 설명하겠습니다. 먼저 브이즈를 호출한 뒤 '습도'라고 말하면 현재 습도 정보를 '{user}'님께 알려줍니다.\"\n",
    "elif '8' in answer:\n",
    "    answer = f\"\"\n",
    "elif '9' in answer:\n",
    "    answer = f\"\"\n",
    "elif '10' in answer:\n",
    "    answer = f\"\"\n",
    "\n",
    "print(\"최적 답변:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e000d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "습도는 어떻게 알 수 있어?\n",
      "질문: 습도는 어떻게 알 수 있어?\n",
      "응답: 7\n",
      "시작 위치 스코어: 6.810416221618652\n",
      "끝 위치 스코어: 4.795325756072998\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# 한국어 Electra 모델과 토크나이저 불러오기\n",
    "model_name = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "model = ElectraForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# 예시 질문과 맥락\n",
    "question = input()\n",
    "context = \"\"\"\n",
    "    날씨라는 호출로 현재 날씨 정보를 가져오는 것을 2로 한다.\n",
    "    카카오톡이라는 호출로 친구에게 카카오톡 메시지를 전송하는 것을 3로 한다. \n",
    "    주차장의 호출로 노상주차 알림 서비스를 제공하는 것을 4로 한다.\n",
    "    시간이라는 호출로 현재 시간을 알려주는 것을 5로 한다.\n",
    "    온도라는 호출로 현재 온도에 대해서 알려주는 것을 6로 한다.\n",
    "    습도라는 호출로 현재 습도에 대해서 알려주는 것을 7로 한다.\"\"\"\n",
    "\n",
    "# 직접 모델 사용\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "\n",
    "# argmax를 이용해 context에서 응답의 시작일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# argmax를 이용해 context에서 응답의 끝일 확률이 가장 높은 토큰의 위치를 반환\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "# 토큰화 결과로부터 input_ids만 추출\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "# input_ids에서 응답에 해당하는 id를 가져와 토큰으로 변환하고 다시 문자열로 변환\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "# 스코어 출력\n",
    "print(\"질문:\", question)\n",
    "print(\"응답:\", answer)\n",
    "print(\"시작 위치 스코어:\", float(torch.max(answer_start_scores)))\n",
    "print(\"끝 위치 스코어:\", float(torch.max(answer_end_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426f9989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "     -------------------------------------- 521.2/521.2 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.8/134.8 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting huggingface-hub>=0.18.0\n",
      "  Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-win_amd64.whl (24.6 MB)\n",
      "     ---------------------------------------- 24.6/24.6 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "     -------------------------------------- 115.3/115.3 kB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gjaischool1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 huggingface-hub-0.19.4 multiprocess-0.70.15 pyarrow-14.0.1 pyarrow-hotfix-0.6 xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66d7e3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features:   0%|                                                    | 0/87599 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'is_impossible'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\gjaischool1\\anaconda3\\lib\\multiprocessing\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\gjaischool1\\anaconda3\\lib\\multiprocessing\\pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"C:\\Users\\gjaischool1\\anaconda3\\lib\\site-packages\\transformers\\data\\processors\\squad.py\", line 108, in squad_convert_example_to_features\n    if is_training and not example.is_impossible:\nAttributeError: 'dict' object has no attribute 'is_impossible'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m train_examples \u001b[38;5;241m=\u001b[39m train_examples\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 데이터셋 전처리\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43msquad_convert_examples_to_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_query_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# DataLoader 설정\u001b[39;00m\n\u001b[0;32m     35\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\data\\processors\\squad.py:376\u001b[0m, in \u001b[0;36msquad_convert_examples_to_features\u001b[1;34m(examples, tokenizer, max_seq_length, doc_stride, max_query_length, is_training, padding_strategy, return_dataset, threads, tqdm_enabled)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(threads, initializer\u001b[38;5;241m=\u001b[39msquad_convert_example_to_features_init, initargs\u001b[38;5;241m=\u001b[39m(tokenizer,)) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m    368\u001b[0m     annotate_ \u001b[38;5;241m=\u001b[39m partial(\n\u001b[0;32m    369\u001b[0m         squad_convert_example_to_features,\n\u001b[0;32m    370\u001b[0m         max_seq_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m         is_training\u001b[38;5;241m=\u001b[39mis_training,\n\u001b[0;32m    375\u001b[0m     )\n\u001b[1;32m--> 376\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotate_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconvert squad examples to features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m new_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    386\u001b[0m unique_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000000000\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\pool.py:423\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    415\u001b[0m result \u001b[38;5;241m=\u001b[39m IMapIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taskqueue\u001b[38;5;241m.\u001b[39mput(\n\u001b[0;32m    417\u001b[0m     (\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guarded_task_generation(result\u001b[38;5;241m.\u001b[39m_job,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m         result\u001b[38;5;241m.\u001b[39m_set_length\n\u001b[0;32m    422\u001b[0m     ))\n\u001b[1;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (item \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m chunk)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'is_impossible'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ElectraForQuestionAnswering, ElectraTokenizer, AdamW\n",
    "from transformers.data.processors.squad import SquadV1Processor, squad_convert_examples_to_features\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 모델 및 토크나이저 로딩\n",
    "model_name = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "model = ElectraForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Squad 데이터셋 다운로드\n",
    "squad_dataset = load_dataset(\"squad\")\n",
    "\n",
    "# 훈련 데이터셋 가져오기\n",
    "train_examples = squad_dataset[\"train\"]\n",
    "# Squad 데이터셋 로딩\n",
    "squad_processor = SquadV1Processor()\n",
    "train_examples = train_examples\n",
    "\n",
    "# 데이터셋 전처리\n",
    "train_features = squad_convert_examples_to_features(\n",
    "    examples=train_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=384,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=True,\n",
    "    return_dataset=\"pt\"\n",
    ")\n",
    "\n",
    "# DataLoader 설정\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_features, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # DataLoader에서 미니배치 순회\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        # Forward pass 및 손실 계산\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass 및 가중치 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")\n",
    "\n",
    "# 학습된 모델 저장\n",
    "model.save_pretrained(\"custom_model\")\n",
    "tokenizer.save_pretrained(\"custom_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff563ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at C:\\Users\\gjaischool1\\Desktop\\project\\vuide\\squad\\ko\\ko.py or any data file in the same directory. Couldn't find 'squad/ko' on the Hugging Face Hub either: FileNotFoundError: Dataset 'squad/ko' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# KorQuAD 데이터셋 로딩\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquad/ko\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 훈련 데이터셋 가져오기\u001b[39;00m\n\u001b[0;32m      7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py:2128\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   2123\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2124\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2125\u001b[0m )\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2128\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2129\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2130\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2131\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2132\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2133\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2134\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2135\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2136\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2137\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2138\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2139\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2140\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2141\u001b[0m )\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py:1814\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1812\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1813\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 1814\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1823\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py:1507\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1506\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m-> 1507\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1508\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1509\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1510\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1511\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at C:\\Users\\gjaischool1\\Desktop\\project\\vuide\\squad\\ko\\ko.py or any data file in the same directory. Couldn't find 'squad/ko' on the Hugging Face Hub either: FileNotFoundError: Dataset 'squad/ko' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# KorQuAD 데이터셋 로딩\n",
    "dataset = load_dataset(\"squad/ko\")\n",
    "\n",
    "# 훈련 데이터셋 가져오기\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# 데이터셋 샘플 출력\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c075e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zipfile36\n",
      "  Downloading zipfile36-0.1.3-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: zipfile36\n",
      "Successfully installed zipfile36-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install zipfile36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2540d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# GitHub에서 다운로드할 ZIP 파일들의 URL 리스트\n",
    "zip_urls = [\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_00.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_01.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_02.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_03.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_04.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_05.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_06.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_07.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_08.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_09.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_10.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_11.zip\",\n",
    "    \"https://github.com/korquad/korquad.github.io/raw/master/dataset/KorQuAD_2.1/train/KorQuAD_2.1_train_12.zip\"\n",
    "]\n",
    "\n",
    "# 압축 해제된 파일들이 저장될 디렉토리 생성\n",
    "output_dir = \"KorQuAD_2.1_train\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ZIP 파일 다운로드 및 압축 해제\n",
    "for zip_url in zip_urls:\n",
    "    response = requests.get(zip_url)\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "        z.extractall(output_dir)\n",
    "\n",
    "print(\"ZIP 파일들을 다운로드하고 압축 해제했습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
